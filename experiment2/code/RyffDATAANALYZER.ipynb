{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f1c188-b604-4762-90b6-59d5dd5c7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel, ttest_ind\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, ttk\n",
    "from tkinter import Text, Listbox, Scrollbar, END, EXTENDED\n",
    "from pathlib import Path\n",
    "import webbrowser\n",
    "import tempfile\n",
    "import html\n",
    "\n",
    "class RyffAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"AI Wellbeing - Ryff Scale Analysis Tool\")\n",
    "        self.root.geometry(\"1000x700\")\n",
    "        \n",
    "        # Ryff Scale Configuration\n",
    "        self.scale_points = 7  # 7-point scale (1 to 7)\n",
    "        \n",
    "        # Subscale definitions (Ryff 42-item version)\n",
    "        self.subscales = {\n",
    "            'Autonomy': [1, 13, 24, 35, 41, 10, 21],\n",
    "            'Environmental_Mastery': [3, 15, 26, 36, 42, 12, 23],\n",
    "            'Personal_Growth': [5, 17, 28, 37, 2, 14, 25],\n",
    "            'Positive_Relations': [7, 18, 30, 38, 4, 16, 27],\n",
    "            'Purpose_in_Life': [9, 20, 32, 39, 6, 29, 33],\n",
    "            'Self_Acceptance': [11, 22, 34, 40, 8, 19, 31]\n",
    "        }\n",
    "        \n",
    "        # Reverse-scored items (as per standard Ryff 42-item scoring)\n",
    "        self.reverse_items = [1, 2, 3, 4, 6, 7, 11, 13, 17, 20, 21, 22, 23, \n",
    "                             27, 29, 31, 35, 36, 37, 38, 40]\n",
    "        \n",
    "        # Data storage\n",
    "        self.loaded_files = {}\n",
    "        self.processed_data = {}\n",
    "        self.file_labels = {}  # Store user-assigned labels\n",
    "        \n",
    "        self.setup_gui()\n",
    "    \n",
    "    def setup_gui(self):\n",
    "        # Create notebook for tabs\n",
    "        notebook = ttk.Notebook(self.root)\n",
    "        notebook.pack(fill='both', expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Tab 1: File Management\n",
    "        file_frame = ttk.Frame(notebook)\n",
    "        notebook.add(file_frame, text=\"File Management\")\n",
    "        \n",
    "        ttk.Button(file_frame, text=\"Load JSON Files\", \n",
    "                  command=self.load_files).pack(pady=5)\n",
    "        \n",
    "        # File list with labels (with multiple selection)\n",
    "        list_frame = ttk.Frame(file_frame)\n",
    "        list_frame.pack(fill='both', expand=True, pady=5)\n",
    "        \n",
    "        ttk.Label(list_frame, text=\"Loaded Files (Ctrl+Click for multiple selection):\").pack(anchor='w')\n",
    "        self.file_listbox = Listbox(list_frame, height=8, selectmode=EXTENDED)\n",
    "        self.file_listbox.pack(fill='both', expand=True)\n",
    "        \n",
    "        # Labeling section\n",
    "        label_frame = ttk.Frame(file_frame)\n",
    "        label_frame.pack(fill='x', pady=5)\n",
    "        \n",
    "        ttk.Label(label_frame, text=\"Label selected file(s) as:\").pack(side='left')\n",
    "        self.label_entry = ttk.Entry(label_frame, width=20)\n",
    "        self.label_entry.pack(side='left', padx=5)\n",
    "        ttk.Button(label_frame, text=\"Set Label\", command=self.set_file_label).pack(side='left')\n",
    "        \n",
    "        # Management buttons\n",
    "        mgmt_frame = ttk.Frame(file_frame)\n",
    "        mgmt_frame.pack(fill='x', pady=5)\n",
    "        \n",
    "        ttk.Button(mgmt_frame, text=\"Remove Selected File(s)\", \n",
    "                  command=self.remove_files).pack(side='left', padx=5)\n",
    "        ttk.Button(mgmt_frame, text=\"Clear All\", \n",
    "                  command=self.cleanup).pack(side='left', padx=5)\n",
    "        ttk.Button(mgmt_frame, text=\"Show File Labels\", \n",
    "                  command=self.show_labels).pack(side='left', padx=5)\n",
    "        \n",
    "        # Tab 2: Analysis\n",
    "        analysis_frame = ttk.Frame(notebook)\n",
    "        notebook.add(analysis_frame, text=\"Analysis\")\n",
    "        \n",
    "        # Analysis buttons\n",
    "        button_frame = ttk.Frame(analysis_frame)\n",
    "        button_frame.pack(fill='x', pady=5)\n",
    "        \n",
    "        buttons = [\n",
    "            (\"1. Process Ryff Scores\", self.process_ryff_scores),\n",
    "            (\"2. Check Internal Consistency\", self.check_consistency),\n",
    "            (\"3. Compare Groups A vs B\", self.compare_groups),\n",
    "            (\"4. Cross-File Similarity\", self.analyze_similarity),\n",
    "            (\"5. Error Analysis\", self.error_analysis)\n",
    "        ]\n",
    "        \n",
    "        for i, (text, command) in enumerate(buttons):\n",
    "            row = i // 2\n",
    "            col = i % 2\n",
    "            btn = ttk.Button(button_frame, text=text, command=command)\n",
    "            btn.grid(row=row, column=col, padx=5, pady=2, sticky='ew')\n",
    "        \n",
    "        button_frame.grid_columnconfigure(0, weight=1)\n",
    "        button_frame.grid_columnconfigure(1, weight=1)\n",
    "        \n",
    "        # Export buttons\n",
    "        export_frame = ttk.Frame(analysis_frame)\n",
    "        export_frame.pack(fill='x', pady=5)\n",
    "        \n",
    "        ttk.Button(export_frame, text=\"Save as TXT\", \n",
    "                  command=self.save_as_txt).pack(side='left', padx=5)\n",
    "        ttk.Button(export_frame, text=\"Save as HTML\", \n",
    "                  command=self.save_as_html).pack(side='left', padx=5)\n",
    "        ttk.Button(export_frame, text=\"Save as PDF\", \n",
    "                  command=self.save_as_pdf).pack(side='left', padx=5)\n",
    "        \n",
    "        # Results display\n",
    "        results_frame = ttk.Frame(analysis_frame)\n",
    "        results_frame.pack(fill='both', expand=True, pady=5)\n",
    "        \n",
    "        self.results_text = Text(results_frame, height=20, wrap='word', font=('Consolas', 10))\n",
    "        scrollbar = Scrollbar(results_frame, orient=\"vertical\", command=self.results_text.yview)\n",
    "        self.results_text.configure(yscrollcommand=scrollbar.set)\n",
    "        \n",
    "        self.results_text.pack(side='left', fill='both', expand=True)\n",
    "        scrollbar.pack(side='right', fill='y')\n",
    "        \n",
    "        # Clear results button\n",
    "        ttk.Button(analysis_frame, text=\"Clear Results\", \n",
    "                  command=self.clear_results).pack(pady=5)\n",
    "    \n",
    "    def load_files(self):\n",
    "        \"\"\"Load JSON files containing Ryff scale responses\"\"\"\n",
    "        try:\n",
    "            files = filedialog.askopenfilenames(\n",
    "                title=\"Select JSON files\",\n",
    "                filetypes=[(\"JSON files\", \"*.json\")]\n",
    "            )\n",
    "            \n",
    "            # Check if user cancelled the dialog\n",
    "            if not files:\n",
    "                return\n",
    "            \n",
    "            loaded_count = 0\n",
    "            for file_path in files:\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        data = json.load(f)\n",
    "                    \n",
    "                    filename = os.path.basename(file_path)\n",
    "                    # Handle duplicate filenames by adding a suffix\n",
    "                    original_filename = filename\n",
    "                    counter = 1\n",
    "                    while filename in self.loaded_files:\n",
    "                        name, ext = os.path.splitext(original_filename)\n",
    "                        filename = f\"{name}_{counter}{ext}\"\n",
    "                        counter += 1\n",
    "                    \n",
    "                    self.loaded_files[filename] = data\n",
    "                    self.file_labels[filename] = \"unlabeled\"\n",
    "                    loaded_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    messagebox.showerror(\"Error\", f\"Failed to load {file_path}: {str(e)}\")\n",
    "            \n",
    "            self.update_file_list()\n",
    "            if loaded_count > 0:\n",
    "                self.log_result(f\"Successfully loaded {loaded_count} files.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Unexpected error during file loading: {str(e)}\")\n",
    "    \n",
    "    def set_file_label(self):\n",
    "        \"\"\"Set label for selected files\"\"\"\n",
    "        try:\n",
    "            selections = self.file_listbox.curselection()\n",
    "            if not selections:\n",
    "                messagebox.showwarning(\"Warning\", \"Please select one or more files first!\")\n",
    "                return\n",
    "            \n",
    "            label = self.label_entry.get().strip()\n",
    "            \n",
    "            if not label:\n",
    "                messagebox.showwarning(\"Warning\", \"Please enter a label!\")\n",
    "                return\n",
    "            \n",
    "            filenames = list(self.loaded_files.keys())\n",
    "            labeled_files = []\n",
    "            \n",
    "            for selection in selections:\n",
    "                if selection < len(filenames):  # Safety check\n",
    "                    filename = filenames[selection]\n",
    "                    self.file_labels[filename] = label\n",
    "                    labeled_files.append(filename)\n",
    "            \n",
    "            self.update_file_list()\n",
    "            self.label_entry.delete(0, tk.END)\n",
    "            self.log_result(f\"Labeled {len(labeled_files)} files as '{label}': {', '.join(labeled_files)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Error setting file labels: {str(e)}\")\n",
    "    \n",
    "    def update_file_list(self):\n",
    "        \"\"\"Update the file listbox with labels\"\"\"\n",
    "        self.file_listbox.delete(0, tk.END)\n",
    "        for filename in self.loaded_files.keys():\n",
    "            label = self.file_labels.get(filename, \"unlabeled\")\n",
    "            display_text = f\"{filename} [{label}]\"\n",
    "            self.file_listbox.insert(tk.END, display_text)\n",
    "    \n",
    "    def show_labels(self):\n",
    "        \"\"\"Show all file labels\"\"\"\n",
    "        self.log_result(\"\\n=== FILE LABELS ===\")\n",
    "        for filename, label in self.file_labels.items():\n",
    "            self.log_result(f\"{filename}: {label}\")\n",
    "    \n",
    "    def remove_files(self):\n",
    "        \"\"\"Remove selected files from analysis\"\"\"\n",
    "        try:\n",
    "            selections = self.file_listbox.curselection()\n",
    "            if not selections:\n",
    "                messagebox.showwarning(\"Warning\", \"Please select one or more files first!\")\n",
    "                return\n",
    "            \n",
    "            filenames = list(self.loaded_files.keys())\n",
    "            to_remove = []\n",
    "            \n",
    "            for selection in reversed(selections):  # Remove in reverse order to maintain indices\n",
    "                if selection < len(filenames):  # Safety check\n",
    "                    filename = filenames[selection]\n",
    "                    to_remove.append(filename)\n",
    "                    del self.loaded_files[filename]\n",
    "                    if filename in self.file_labels:\n",
    "                        del self.file_labels[filename]\n",
    "                    if filename in self.processed_data:\n",
    "                        del self.processed_data[filename]\n",
    "            \n",
    "            self.update_file_list()\n",
    "            self.log_result(f\"Removed {len(to_remove)} files: {', '.join(to_remove)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Error removing files: {str(e)}\")\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clear all data\"\"\"\n",
    "        try:\n",
    "            self.loaded_files.clear()\n",
    "            self.processed_data.clear()\n",
    "            self.file_labels.clear()\n",
    "            self.update_file_list()\n",
    "            self.clear_results()\n",
    "            self.log_result(\"All data cleared.\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Error clearing data: {str(e)}\")\n",
    "    \n",
    "    def clear_results(self):\n",
    "        \"\"\"Clear the results text area\"\"\"\n",
    "        self.results_text.delete(1.0, tk.END)\n",
    "    \n",
    "    def get_results_text(self):\n",
    "        \"\"\"Get all text from results area\"\"\"\n",
    "        return self.results_text.get(1.0, tk.END)\n",
    "    \n",
    "    def save_as_txt(self):\n",
    "        \"\"\"Save results as TXT file\"\"\"\n",
    "        try:\n",
    "            content = self.get_results_text()\n",
    "            if not content.strip():\n",
    "                messagebox.showwarning(\"Warning\", \"No results to save!\")\n",
    "                return\n",
    "            \n",
    "            filename = filedialog.asksaveasfilename(\n",
    "                defaultextension=\".txt\",\n",
    "                filetypes=[(\"Text files\", \"*.txt\"), (\"All files\", \"*.*\")]\n",
    "            )\n",
    "            \n",
    "            if filename:\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "                messagebox.showinfo(\"Success\", f\"Results saved to {filename}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to save file: {str(e)}\")\n",
    "    \n",
    "    def save_as_html(self):\n",
    "        \"\"\"Save results as HTML file\"\"\"\n",
    "        try:\n",
    "            content = self.get_results_text()\n",
    "            if not content.strip():\n",
    "                messagebox.showwarning(\"Warning\", \"No results to save!\")\n",
    "                return\n",
    "            \n",
    "            filename = filedialog.asksaveasfilename(\n",
    "                defaultextension=\".html\",\n",
    "                filetypes=[(\"HTML files\", \"*.html\"), (\"All files\", \"*.*\")]\n",
    "            )\n",
    "            \n",
    "            if filename:\n",
    "                html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Ryff Scale Analysis Results</title>\n",
    "    <style>\n",
    "        body {{ font-family: 'Courier New', monospace; white-space: pre-wrap; margin: 20px; }}\n",
    "        .header {{ color: #2c3e50; font-weight: bold; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "{html.escape(content)}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(html_content)\n",
    "                messagebox.showinfo(\"Success\", f\"Results saved to {filename}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to save file: {str(e)}\")\n",
    "    \n",
    "    def save_as_pdf(self):\n",
    "        \"\"\"Save results as PDF file\"\"\"\n",
    "        try:\n",
    "            content = self.get_results_text()\n",
    "            if not content.strip():\n",
    "                messagebox.showwarning(\"Warning\", \"No results to save!\")\n",
    "                return\n",
    "            \n",
    "            filename = filedialog.asksaveasfilename(\n",
    "                defaultextension=\".pdf\",\n",
    "                filetypes=[(\"PDF files\", \"*.pdf\"), (\"All files\", \"*.*\")]\n",
    "            )\n",
    "            \n",
    "            if filename:\n",
    "                try:\n",
    "                    from reportlab.lib.pagesizes import letter\n",
    "                    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "                    from reportlab.lib.styles import getSampleStyleSheet\n",
    "                    from reportlab.lib.units import inch\n",
    "                    \n",
    "                    doc = SimpleDocTemplate(filename, pagesize=letter)\n",
    "                    styles = getSampleStyleSheet()\n",
    "                    story = []\n",
    "                    \n",
    "                    # Split content into lines and create paragraphs\n",
    "                    lines = content.split('\\n')\n",
    "                    for line in lines:\n",
    "                        if line.strip():\n",
    "                            # Use a monospace style for consistent formatting\n",
    "                            para = Paragraph(html.escape(line), styles['Code'])\n",
    "                            story.append(para)\n",
    "                        else:\n",
    "                            story.append(Spacer(1, 0.1*inch))\n",
    "                    \n",
    "                    doc.build(story)\n",
    "                    messagebox.showinfo(\"Success\", f\"Results saved to {filename}\")\n",
    "                    \n",
    "                except ImportError:\n",
    "                    messagebox.showerror(\"Error\", \"PDF export requires reportlab library. Install with: pip install reportlab\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to save PDF: {str(e)}\")\n",
    "    \n",
    "    def reverse_score(self, value):\n",
    "        \"\"\"Apply reverse scoring formula for a 1-7 Likert scale.\"\"\"\n",
    "        if value is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            numeric_value = float(value)\n",
    "            if not (1 <= numeric_value <= self.scale_points) or not numeric_value.is_integer():\n",
    "                return None\n",
    "            \n",
    "            # For a 7-point scale, this is 8 - value\n",
    "            return (self.scale_points + 1) - int(numeric_value)\n",
    "            \n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    def safe_std(self, data, ddof=1):\n",
    "        \"\"\"Safely calculate standard deviation, handling edge cases\"\"\"\n",
    "        if not data or len(data) <= ddof: # Needs at least 2 points for ddof=1\n",
    "            return None\n",
    "        try:\n",
    "            return np.std(data, ddof=ddof)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def safe_mean(self, data):\n",
    "        \"\"\"Safely calculate mean, handling edge cases\"\"\"\n",
    "        if not data:\n",
    "            return None\n",
    "        try:\n",
    "            return np.mean(data)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def process_single_file(self, data, filename):\n",
    "        \"\"\"\n",
    "        Process a single JSON file for Ryff scoring.\n",
    "        Calculates total score, subscale sums, and subscale averages.\n",
    "        \"\"\"\n",
    "        processed = {\n",
    "            'filename': filename,\n",
    "            'raw_scores': {},\n",
    "            'reversed_scores': {},\n",
    "            'subscale_sums': {},\n",
    "            'subscale_averages': {},\n",
    "            'global_ryff_total_score': None, # Initialize as None, set if valid\n",
    "            'errors': [],\n",
    "            'invalid_entries': [],\n",
    "            'valid': True\n",
    "        }\n",
    "        \n",
    "        # Process each item (1 to 42)\n",
    "        error_count = 0\n",
    "        valid_reversed_scores_for_total = {} # Collect valid reversed scores for final sum\n",
    "        \n",
    "        for item_num in range(1, 43):\n",
    "            item_key = str(item_num)\n",
    "            \n",
    "            if item_key not in data:\n",
    "                processed['errors'].append(f\"Missing item {item_num}\")\n",
    "                processed['invalid_entries'].append(f\"Item {item_num}: missing\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "            \n",
    "            raw_value = data[item_key]\n",
    "            \n",
    "            if raw_value is None:\n",
    "                processed['errors'].append(f\"Null value for item {item_num}\")\n",
    "                processed['invalid_entries'].append(f\"Item {item_num}: null\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "            \n",
    "            if isinstance(raw_value, (list, dict)):\n",
    "                processed['errors'].append(f\"Multiple values (list/dict) for item {item_num}\")\n",
    "                processed['invalid_entries'].append(f\"Item {item_num}: {raw_value}\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                numeric_value = float(raw_value)\n",
    "                if not (1 <= numeric_value <= self.scale_points) or not numeric_value.is_integer():\n",
    "                    processed['errors'].append(f\"Invalid value (out of range 1-{self.scale_points} or not integer) for item {item_num}: {raw_value}\")\n",
    "                    processed['invalid_entries'].append(f\"Item {item_num}: {raw_value}\")\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "                numeric_value = int(numeric_value)\n",
    "            except (ValueError, TypeError):\n",
    "                processed['errors'].append(f\"Non-numeric value for item {item_num}: {raw_value}\")\n",
    "                processed['invalid_entries'].append(f\"Item {item_num}: {raw_value}\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Successfully processed raw item\n",
    "            processed['raw_scores'][item_num] = numeric_value\n",
    "            \n",
    "            # Apply reverse scoring if needed\n",
    "            reversed_val = None\n",
    "            if item_num in self.reverse_items:\n",
    "                reversed_val = self.reverse_score(numeric_value)\n",
    "            else:\n",
    "                reversed_val = numeric_value\n",
    "            \n",
    "            if reversed_val is not None:\n",
    "                processed['reversed_scores'][item_num] = reversed_val\n",
    "                valid_reversed_scores_for_total[item_num] = reversed_val # Add to valid list for total sum\n",
    "            else:\n",
    "                processed['errors'].append(f\"Error during reverse/direct scoring for item {item_num}: {raw_value}\")\n",
    "                processed['invalid_entries'].append(f\"Item {item_num}: Scoring error\")\n",
    "                error_count += 1\n",
    "        \n",
    "        # Mark file as invalid if too many errors\n",
    "        if error_count >= 8:\n",
    "            processed['valid'] = False\n",
    "            return processed\n",
    "        \n",
    "        # Calculate Subscale Sums and Averages\n",
    "        for subscale_name, items in self.subscales.items():\n",
    "            subscale_item_values = []\n",
    "            for item_num in items:\n",
    "                if item_num in processed['reversed_scores']: # Use only successfully reversed items\n",
    "                    subscale_item_values.append(processed['reversed_scores'][item_num])\n",
    "            \n",
    "            if subscale_item_values:\n",
    "                processed['subscale_sums'][subscale_name] = sum(subscale_item_values)\n",
    "                processed['subscale_averages'][subscale_name] = self.safe_mean(subscale_item_values)\n",
    "            else:\n",
    "                processed['subscale_sums'][subscale_name] = None\n",
    "                processed['subscale_averages'][subscale_name] = None\n",
    "        \n",
    "        # Calculate Global Ryff Total Score (sum of all VALID reversed items)\n",
    "        # Ensure that only items successfully processed and reversed contribute to the total\n",
    "        if valid_reversed_scores_for_total:\n",
    "            processed['global_ryff_total_score'] = sum(valid_reversed_scores_for_total.values())\n",
    "        else:\n",
    "            processed['global_ryff_total_score'] = None\n",
    "            processed['valid'] = False # If no valid items for total score, mark as invalid\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def create_scores_table(self, valid_files):\n",
    "        \"\"\"Create formatted tables for scores\"\"\"\n",
    "        self.log_result(\"\\nGLOBAL RYFF TOTAL SCORES TABLE (Sum of valid reversed items):\")\n",
    "        self.log_result(\"=\" * 80)\n",
    "        self.log_result(f\"{'Filename':<30} {'Label':<15} {'Total Score':<12}\")\n",
    "        self.log_result(\"-\" * 80)\n",
    "        \n",
    "        for filename in valid_files:\n",
    "            label = self.file_labels.get(filename, \"unlabeled\")\n",
    "            score = self.processed_data[filename].get('global_ryff_total_score') # Use .get for safety\n",
    "            if score is not None:\n",
    "                self.log_result(f\"{filename:<30} {label:<15} {score:<12.3f}\")\n",
    "            else:\n",
    "                self.log_result(f\"{filename:<30} {label:<15} {'N/A':<12}\")\n",
    "        \n",
    "        self.log_result(f\"\\nSUBSCALE SUMS TABLE:\")\n",
    "        self.log_result(\"=\" * 120)\n",
    "        \n",
    "        # Header for subscale sums\n",
    "        header_sums = f\"{'Filename':<20} {'Label':<10}\"\n",
    "        for subscale in self.subscales.keys():\n",
    "            header_sums += f\" {subscale[:8]:<9}\"\n",
    "        self.log_result(header_sums)\n",
    "        self.log_result(\"-\" * 120)\n",
    "        \n",
    "        # Data rows for subscale sums\n",
    "        for filename in valid_files:\n",
    "            label = self.file_labels.get(filename, \"unlabeled\")\n",
    "            row_sums = f\"{filename:<20} {label:<10}\"\n",
    "            \n",
    "            for subscale in self.subscales.keys():\n",
    "                score = self.processed_data[filename]['subscale_sums'].get(subscale)\n",
    "                if score is not None:\n",
    "                    row_sums += f\" {score:<9.3f}\"\n",
    "                else:\n",
    "                    row_sums += f\" {'N/A':<9}\"\n",
    "            \n",
    "            self.log_result(row_sums)\n",
    "\n",
    "        self.log_result(f\"\\nSUBSCALE AVERAGES TABLE:\")\n",
    "        self.log_result(\"=\" * 120)\n",
    "        \n",
    "        # Header for subscale averages\n",
    "        header_averages = f\"{'Filename':<20} {'Label':<10}\"\n",
    "        for subscale in self.subscales.keys():\n",
    "            header_averages += f\" {subscale[:8]:<9}\"\n",
    "        self.log_result(header_averages)\n",
    "        self.log_result(\"-\" * 120)\n",
    "        \n",
    "        # Data rows for subscale averages\n",
    "        for filename in valid_files:\n",
    "            label = self.file_labels.get(filename, \"unlabeled\")\n",
    "            row_averages = f\"{filename:<20} {label:<10}\"\n",
    "            \n",
    "            for subscale in self.subscales.keys():\n",
    "                score = self.processed_data[filename]['subscale_averages'].get(subscale)\n",
    "                if score is not None:\n",
    "                    row_averages += f\" {score:<9.3f}\"\n",
    "                else:\n",
    "                    row_averages += f\" {'N/A':<9}\"\n",
    "            \n",
    "            self.log_result(row_averages)\n",
    "            \n",
    "    def analyze_subscale_extremes(self, valid_files):\n",
    "        \"\"\"Analyze highest and lowest scoring subscales based on subscale averages.\"\"\"\n",
    "        self.log_result(f\"\\nSUBSCALE EXTREMES ANALYSIS (Based on Subscale Averages):\")\n",
    "        self.log_result(\"=\" * 60)\n",
    "        \n",
    "        self.log_result(\"INDIVIDUAL FILE ANALYSIS:\")\n",
    "        self.log_result(\"-\" * 40)\n",
    "        \n",
    "        for filename in valid_files:\n",
    "            data = self.processed_data[filename]\n",
    "            label = self.file_labels.get(filename, \"unlabeled\")\n",
    "            subscale_averages = data['subscale_averages']\n",
    "            \n",
    "            if subscale_averages:\n",
    "                filtered_averages = {k: v for k, v in subscale_averages.items() if v is not None}\n",
    "                \n",
    "                if filtered_averages:\n",
    "                    highest_subscale = max(filtered_averages.items(), key=lambda x: x[1])\n",
    "                    lowest_subscale = min(filtered_averages.items(), key=lambda x: x[1])\n",
    "                    \n",
    "                    self.log_result(f\"\\n{filename} [{label}]:\")\n",
    "                    self.log_result(f\"  Highest: {highest_subscale[0]} ({highest_subscale[1]:.3f})\")\n",
    "                    self.log_result(f\"  Lowest:  {lowest_subscale[0]} ({lowest_subscale[1]:.3f})\")\n",
    "                    self.log_result(f\"  Range:   {highest_subscale[1] - lowest_subscale[1]:.3f}\")\n",
    "                else:\n",
    "                    self.log_result(f\"\\n{filename} [{label}]: No valid subscale averages to analyze extremes.\")\n",
    "            else:\n",
    "                self.log_result(f\"\\n{filename} [{label}]: No subscale averages available for analysis.\")\n",
    "        \n",
    "        self.log_result(f\"\\nOVERALL ANALYSIS ACROSS ALL FILES:\")\n",
    "        self.log_result(\"-\" * 40)\n",
    "        \n",
    "        subscale_group_averages = {}\n",
    "        for subscale_name in self.subscales.keys():\n",
    "            scores_across_files = []\n",
    "            for filename in valid_files:\n",
    "                subscale_avg = self.processed_data[filename]['subscale_averages'].get(subscale_name)\n",
    "                if subscale_avg is not None:\n",
    "                    scores_across_files.append(subscale_avg)\n",
    "            \n",
    "            if scores_across_files:\n",
    "                subscale_group_averages[subscale_name] = self.safe_mean(scores_across_files)\n",
    "        \n",
    "        if subscale_group_averages:\n",
    "            valid_group_averages = {k: v for k, v in subscale_group_averages.items() if v is not None}\n",
    "            \n",
    "            if valid_group_averages:\n",
    "                highest_overall = max(valid_group_averages.items(), key=lambda x: x[1])\n",
    "                lowest_overall = min(valid_group_averages.items(), key=lambda x: x[1])\n",
    "                \n",
    "                self.log_result(f\"\\nAverage subscale scores across {len(valid_files)} files:\")\n",
    "                for subscale, avg_score in sorted(valid_group_averages.items(), key=lambda x: x[1], reverse=True):\n",
    "                    self.log_result(f\"  {subscale:<20}: {avg_score:.3f}\")\n",
    "                \n",
    "                self.log_result(f\"\\nOVERALL EXTREMES:\")\n",
    "                self.log_result(f\"  Highest scoring subscale: {highest_overall[0]} ({highest_overall[1]:.3f})\")\n",
    "                self.log_result(f\"  Lowest scoring subscale:  {lowest_overall[0]} ({lowest_overall[1]:.3f})\")\n",
    "                self.log_result(f\"  Overall range:            {highest_overall[1] - lowest_overall[1]:.3f}\")\n",
    "            else:\n",
    "                self.log_result(\"\\nNo valid overall subscale averages available for extreme analysis.\")\n",
    "        else:\n",
    "            self.log_result(\"\\nNo overall subscale averages available for extreme analysis.\")\n",
    "            \n",
    "    def process_ryff_scores(self):\n",
    "        \"\"\"\n",
    "        Process all loaded files for Ryff scoring, calculating total scores,\n",
    "        subscale sums, and subscale averages for each file.\n",
    "        \"\"\"\n",
    "        if not self.loaded_files:\n",
    "            messagebox.showwarning(\"Warning\", \"No files loaded!\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.log_result(\"=\" * 60)\n",
    "            self.log_result(\"RYFF PSYCHOLOGICAL WELL-BEING SCALE PROCESSING\")\n",
    "            self.log_result(\"=\" * 60)\n",
    "            \n",
    "            self.log_result(\"\\nMETHODOLOGY EXPLANATION:\")\n",
    "            self.log_result(\"The Ryff Scale measures psychological well-being across 6 dimensions.\")\n",
    "            self.log_result(f\"Each dimension is composed of 7 items, rated on a {self.scale_points}-point Likert scale (1=strongly disagree, {self.scale_points}=strongly agree).\")\n",
    "            self.log_result(\"This process calculates scores for each AI response file (subject) based on these items.\")\n",
    "            \n",
    "            self.log_result(\"\\nREVERSE SCORING:\")\n",
    "            self.log_result(f\"Items worded negatively are reverse-scored to ensure higher scores consistently reflect higher well-being.\")\n",
    "            self.log_result(f\"Formula used: ({self.scale_points + 1} - original_score). For a 1-7 scale, this is (8 - original_score).\")\n",
    "            self.log_result(f\"Items identified for reverse-scoring: {sorted(self.reverse_items)}\")\n",
    "            \n",
    "            self.log_result(\"\\nSCORING FORMULAS:\")\n",
    "            self.log_result(\"- **Subscale Sum:** Simple sum of the 7 valid reversed item scores within each specific subscale.\")\n",
    "            self.log_result(\"- **Subscale Average:** Mean (average) of the 7 valid reversed item scores within each specific subscale.\")\n",
    "            self.log_result(f\"- **Global Ryff Total Score:** Simple SUM of all 42 valid individual item scores (after reverse scoring).\")\n",
    "            self.log_result(f\"  - This total score can range from {42 * 1} (all minimums) to {42 * self.scale_points} (all maximums). For a 1-7 scale, this range is {42} to {294}.\")\n",
    "            \n",
    "            self.log_result(\"\\nVALIDITY THRESHOLD:\")\n",
    "            self.log_result(\"Files with 8 or more invalid responses (missing items, non-numeric, out-of-range values, multiple values) are marked as INVALID.\")\n",
    "            self.log_result(\"INVALID files are excluded from statistical comparisons and consistency analyses to ensure reliable results.\")\n",
    "            self.log_result(\"\\nACCEPTABLE ITEM VALUES:\")\n",
    "            self.log_result(f\"Only single integer values from 1 to {self.scale_points} are considered valid responses for individual items.\")\n",
    "            self.log_result(\"-\" * 60)\n",
    "            \n",
    "            valid_files = []\n",
    "            invalid_files = []\n",
    "            \n",
    "            for filename, data in self.loaded_files.items():\n",
    "                processed = self.process_single_file(data, filename)\n",
    "                self.processed_data[filename] = processed\n",
    "                \n",
    "                label = self.file_labels.get(filename, \"unlabeled\")\n",
    "                \n",
    "                if processed['valid']:\n",
    "                    valid_files.append(filename)\n",
    "                    self.log_result(f\"\\n✓ {filename} [{label}] - VALID FILE\")\n",
    "                    score = processed['global_ryff_total_score']\n",
    "                    if score is not None:\n",
    "                        self.log_result(f\"  Calculated Global Ryff Total Score: {score:.3f}\")\n",
    "                    else:\n",
    "                        self.log_result(f\"  Global Ryff Total Score: N/A (Insufficient valid items)\")\n",
    "                    self.log_result(f\"  Number of errors/invalid entries detected: {len(processed['errors'])}\")\n",
    "                    \n",
    "                    self.log_result(\"  Subscale Sums:\")\n",
    "                    for subscale, score_sum in processed['subscale_sums'].items():\n",
    "                        if score_sum is not None:\n",
    "                            self.log_result(f\"    {subscale}: {score_sum:.3f}\")\n",
    "                        else:\n",
    "                            self.log_result(f\"    {subscale}: N/A (Insufficient valid items)\")\n",
    "                    \n",
    "                    self.log_result(\"  Subscale Averages:\")\n",
    "                    for subscale, score_avg in processed['subscale_averages'].items():\n",
    "                        if score_avg is not None:\n",
    "                            self.log_result(f\"    {subscale}: {score_avg:.3f}\")\n",
    "                        else:\n",
    "                            self.log_result(f\"    {subscale}: N/A (Insufficient valid items)\")\n",
    "                else:\n",
    "                    invalid_files.append(filename)\n",
    "                    self.log_result(f\"\\n✗ {filename} [{label}] - INVALID FILE ({len(processed['errors'])} errors)\")\n",
    "                    self.log_result(f\"  This file exceeded the validity threshold (≥8 errors) and will be excluded from further analyses.\")\n",
    "                    if processed['invalid_entries']:\n",
    "                        self.log_result(f\"  Example invalid entries: {', '.join(processed['invalid_entries'][:5])}{'...' if len(processed['invalid_entries']) > 5 else ''}\")\n",
    "            \n",
    "            # Create summary tables at the end\n",
    "            if valid_files:\n",
    "                self.create_scores_table(valid_files)\n",
    "                self.analyze_subscale_extremes(valid_files)\n",
    "            else:\n",
    "                self.log_result(\"\\nNo valid files were processed for summary tables.\")\n",
    "            \n",
    "            self.log_result(f\"\\n\" + \"=\" * 40)\n",
    "            self.log_result(f\"PROCESSING SUMMARY:\")\n",
    "            self.log_result(f\"Total files attempted: {len(self.loaded_files)}\")\n",
    "            self.log_result(f\"Valid files for analysis: {len(valid_files)}\")\n",
    "            self.log_result(f\"Invalid files (excluded): {len(invalid_files)}\")\n",
    "            total_files_sum = len(valid_files) + len(invalid_files)\n",
    "            if total_files_sum > 0:\n",
    "                self.log_result(f\"Success rate: {len(valid_files)/total_files_sum*100:.1f}%\")\n",
    "            else:\n",
    "                self.log_result(\"Success rate: N/A\")\n",
    "            self.log_result(\"=\" * 40)\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Error processing Ryff scores: {str(e)}\")\n",
    "    \n",
    "    def check_consistency(self):\n",
    "        \"\"\"\n",
    "        Check internal consistency for each AI instance.\n",
    "        MODIFIED: Enforces minimum item count, defines global consistency, and provides detailed summary.\n",
    "        \"\"\"\n",
    "        if not self.processed_data:\n",
    "            messagebox.showwarning(\"Warning\", \"Please process Ryff scores first!\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.log_result(\"\\n\" + \"=\" * 60)\n",
    "            self.log_result(\"INTERNAL CONSISTENCY (WITHIN SUBSCALE) ANALYSIS\")\n",
    "            self.log_result(\"=\" * 60)\n",
    "            \n",
    "            self.log_result(\"\\nMETHODOLOGY EXPLANATION:\")\n",
    "            self.log_result(\"This analysis assesses if an AI responded consistently to items measuring the same underlying construct.\")\n",
    "            self.log_result(\"We calculate the Standard Deviation (SD) of the reversed item scores *within each subscale* for *each file*.\")\n",
    "            self.log_result(\"A minimum of 5 valid items (out of 7) is required to calculate the SD for a subscale to ensure the measure is meaningful.\")\n",
    "\n",
    "            self.log_result(\"\\nCONSISTENCY RULES & THRESHOLDS:\")\n",
    "            self.log_result(\"1. **Subscale Inconsistency:** A subscale is flagged if its item response SD > 2.0.\")\n",
    "            self.log_result(\"2. **Global Inconsistency:** An entire file is flagged as 'Globally Inconsistent' if it has MORE THAN 2 inconsistent subscales (i.e., fewer than 4 consistent scales).\")\n",
    "            self.log_result(\"-\" * 60)\n",
    "            \n",
    "            MINIMUM_ITEMS_REQUIRED = 5\n",
    "            GLOBAL_INCONSISTENCY_THRESHOLD = 2 # A file is inconsistent if it has >2 inconsistent scales.\n",
    "            \n",
    "            inconsistency_summary = {i: 0 for i in range(1, 7)} # {1: 0, 2: 0, ... 6: 0}\n",
    "            globally_inconsistent_files_count = 0\n",
    "            \n",
    "            for filename, data in self.processed_data.items():\n",
    "                if not data['valid']:\n",
    "                    self.log_result(f\"\\n{filename} [{self.file_labels.get(filename, 'unlabeled')}]: SKIPPED (marked as INVALID in processing stage).\")\n",
    "                    continue\n",
    "                \n",
    "                label = self.file_labels.get(filename, \"unlabeled\")\n",
    "                subscale_consistency_issues = {}\n",
    "                \n",
    "                self.log_result(f\"\\nANALYZING FILE: {filename} [{label}]\")\n",
    "                self.log_result(f\"  Subscale Item Response Standard Deviations:\")\n",
    "                \n",
    "                for subscale_name, items in self.subscales.items():\n",
    "                    subscale_item_reversed_scores = []\n",
    "                    for item_num in items:\n",
    "                        if item_num in data['reversed_scores']:\n",
    "                            subscale_item_reversed_scores.append(data['reversed_scores'][item_num])\n",
    "                    \n",
    "                    if len(subscale_item_reversed_scores) < MINIMUM_ITEMS_REQUIRED:\n",
    "                        msg = f\"N/A (Cannot calculate. Found {len(subscale_item_reversed_scores)}/{len(items)} valid items, need at least {MINIMUM_ITEMS_REQUIRED}).\"\n",
    "                        self.log_result(f\"    {subscale_name:<20}: {msg}\")\n",
    "                        continue\n",
    "                    \n",
    "                    sub_sd = self.safe_std(subscale_item_reversed_scores, ddof=1)\n",
    "                    \n",
    "                    if sub_sd is None:\n",
    "                        self.log_result(f\"    {subscale_name:<20}: N/A (Cannot calculate SD).\")\n",
    "                        continue\n",
    "                    \n",
    "                    marker = \"✓\"\n",
    "                    # Use a general threshold of 2.0 to flag any inconsistency\n",
    "                    if sub_sd > 2.0:\n",
    "                        marker = \"⚠️\"\n",
    "                        subscale_consistency_issues[subscale_name] = sub_sd\n",
    "                    \n",
    "                    scores_tuple = tuple(subscale_item_reversed_scores)\n",
    "                    self.log_result(f\"    {marker} {subscale_name:<20}: {sub_sd:.3f} --- Scores: {scores_tuple}\")\n",
    "                \n",
    "                # Assess the file's overall consistency based on the number of flagged subscales\n",
    "                num_inconsistent = len(subscale_consistency_issues)\n",
    "                if num_inconsistent > 0:\n",
    "                    inconsistency_summary[num_inconsistent] += 1\n",
    "                \n",
    "                if num_inconsistent > GLOBAL_INCONSISTENCY_THRESHOLD:\n",
    "                    globally_inconsistent_files_count += 1\n",
    "                    self.log_result(f\"  ⚠️ This file is considered GLOBALLY INCONSISTENT ({num_inconsistent} inconsistent scales found, exceeds threshold of {GLOBAL_INCONSISTENCY_THRESHOLD}).\")\n",
    "                else:\n",
    "                    self.log_result(f\"  ✓ This file is considered GLOBALLY CONSISTENT ({num_inconsistent} inconsistent scales found).\")\n",
    "            \n",
    "            # --- OVERALL SUMMARY ---\n",
    "            valid_analyzed = len([f for f in self.processed_data.values() if f['valid']])\n",
    "            self.log_result(f\"\\n\" + \"=\" * 60)\n",
    "            self.log_result(f\"OVERALL CONSISTENCY SUMMARY\")\n",
    "            self.log_result(\"=\" * 60)\n",
    "            self.log_result(f\"Total valid files analyzed for consistency: {valid_analyzed}\")\n",
    "            \n",
    "            self.log_result(\"\\nBreakdown by number of inconsistent scales per file:\")\n",
    "            any_found = False\n",
    "            for num, count in inconsistency_summary.items():\n",
    "                if count > 0:\n",
    "                    self.log_result(f\"  - Files with exactly {num} inconsistent scale(s): {count}\")\n",
    "                    any_found = True\n",
    "            if not any_found and valid_analyzed > 0:\n",
    "                 self.log_result(\"  ✓ No files were found with any inconsistent scales.\")\n",
    "            \n",
    "            self.log_result(f\"\\nRule: A file is 'Globally Inconsistent' if it has >{GLOBAL_INCONSISTENCY_THRESHOLD} inconsistent scales (i.e., fewer than 4 consistent scales).\")\n",
    "            self.log_result(f\"Total files flagged as Globally Inconsistent: {globally_inconsistent_files_count}\")\n",
    "            \n",
    "            if valid_analyzed > 0:\n",
    "                consistent_files_count = valid_analyzed - globally_inconsistent_files_count\n",
    "                consistency_rate = (consistent_files_count / valid_analyzed) * 100\n",
    "                self.log_result(f\"Overall File Consistency Rate (files deemed 'Globally Consistent'): {consistency_rate:.1f}%\")\n",
    "            else:\n",
    "                self.log_result(\"Overall File Consistency Rate: N/A (no valid files)\")\n",
    "            self.log_result(\"=\" * 60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Error checking consistency: {str(e)}\")\n",
    "\n",
    "    def determine_comparison_type(self, n_a, n_b):\n",
    "        \"\"\"Helper to determine the type of statistical comparison needed based on sample sizes.\"\"\"\n",
    "        if n_a == 1 and n_b > 1:\n",
    "            return \"one_sample_a_vs_multi_b\"\n",
    "        elif n_a > 1 and n_b == 1:\n",
    "            return \"multi_a_vs_one_sample_b\"\n",
    "        elif n_a > 1 and n_b > 1:\n",
    "            return \"two_sample_multi_vs_multi\"\n",
    "        else:  # n_a == 1 and n_b == 1\n",
    "            return \"both_single\"\n",
    "            \n",
    "    def calculate_cohens_d(self, mean_sample, sd_sample, mean_reference, sd_reference=None, comparison_type=None):\n",
    "        \"\"\"\n",
    "        Calculates Cohen's d based on the specified comparison type.\n",
    "        mean_sample: Mean of the group that is considered the 'sample' (the one with SD)\n",
    "        sd_sample: SD of the group that is considered the 'sample'\n",
    "        mean_reference: Mean of the reference group/single value\n",
    "        sd_reference: SD of the reference group (only applicable for two_sample_multi_vs_multi)\n",
    "        comparison_type: Type of comparison (string)\n",
    "        \"\"\"\n",
    "        \n",
    "        diff = abs(mean_sample - mean_reference)\n",
    "        \n",
    "        if comparison_type in [\"one_sample_a_vs_multi_b\", \"multi_a_vs_one_sample_b\"]:\n",
    "            # For one-sample test, denominator is the SD of the 'sample' group (the one with N > 1)\n",
    "            if sd_sample is not None and sd_sample > 0:\n",
    "                return diff / sd_sample\n",
    "            return 0.0\n",
    "        \n",
    "        elif comparison_type == \"two_sample_multi_vs_multi\":\n",
    "            # For Welch's t-test, use the square root of the average of the two variances\n",
    "            if sd_sample is not None and sd_reference is not None and (sd_sample > 0 or sd_reference > 0):\n",
    "                combined_std = np.sqrt((sd_sample**2 + sd_reference**2) / 2)\n",
    "                return diff / combined_std if combined_std > 0 else 0.0\n",
    "            return 0.0\n",
    "        \n",
    "        # 'both_single' case handles effect size estimation separately in compare_groups\n",
    "        return 0.0\n",
    "        \n",
    "    def perform_subscale_comparison(self, group_a_files, group_b_files, group_a_label, group_b_label):\n",
    "        \"\"\"\n",
    "        Performs detailed subscale comparison between groups, including statistical tests\n",
    "        and Cohen's d for each subscale. This is called from compare_groups.\n",
    "        \"\"\"\n",
    "        self.log_result(f\"\\nSUBSCALE LEVEL COMPARISON:\")\n",
    "        self.log_result(\"-\" * 40)\n",
    "        \n",
    "        significant_subscales = [] # Stores names of subscales with p < 0.05\n",
    "        \n",
    "        for subscale_name in self.subscales.keys():\n",
    "            # Get the computed subscale AVERAGE scores for each file in both groups\n",
    "            group_a_subscale_averages = []\n",
    "            group_b_subscale_averages = []\n",
    "            \n",
    "            for filename in group_a_files:\n",
    "                if filename in self.processed_data:\n",
    "                    subscale_avg = self.processed_data[filename]['subscale_averages'].get(subscale_name)\n",
    "                    if subscale_avg is not None:\n",
    "                        group_a_subscale_averages.append(subscale_avg)\n",
    "            \n",
    "            for filename in group_b_files:\n",
    "                if filename in self.processed_data:\n",
    "                    subscale_avg = self.processed_data[filename]['subscale_averages'].get(subscale_name)\n",
    "                    if subscale_avg is not None:\n",
    "                        group_b_subscale_averages.append(subscale_avg)\n",
    "            \n",
    "            self.log_result(f\"\\n  Subscale: {subscale_name}\")\n",
    "            \n",
    "            # --- Descriptive Statistics for Subscales ---\n",
    "            a_mean_sub = self.safe_mean(group_a_subscale_averages)\n",
    "            b_mean_sub = self.safe_mean(group_b_subscale_averages)\n",
    "            \n",
    "            a_std_sub = self.safe_std(group_a_subscale_averages, ddof=1)\n",
    "            b_std_sub = self.safe_std(group_b_subscale_averages, ddof=1)\n",
    "            \n",
    "            if a_mean_sub is None or b_mean_sub is None or len(group_a_subscale_averages) == 0 or len(group_b_subscale_averages) == 0:\n",
    "                self.log_result(f\"    Insufficient valid data for this subscale comparison. Group A N={len(group_a_subscale_averages)}, Group B N={len(group_b_subscale_averages)}.\")\n",
    "                continue # Skip to next subscale\n",
    "            \n",
    "            self.log_result(f\"    Group A ({group_a_label}): Mean={a_mean_sub:.3f}{f', SD={a_std_sub:.3f}' if a_std_sub is not None else ''} (n={len(group_a_subscale_averages)})\")\n",
    "            self.log_result(f\"    Group B ({group_b_label}): Mean={b_mean_sub:.3f}{f', SD={b_std_sub:.3f}' if b_std_sub is not None else ''} (n={len(group_b_subscale_averages)})\")\n",
    "            self.log_result(f\"    Absolute difference: {abs(a_mean_sub - b_mean_sub):.3f}\")\n",
    "            \n",
    "            t_stat = p_val = df = effect_size = None\n",
    "            test_type = \"Not applicable\" # Default if no test is performed\n",
    "            \n",
    "            # Determine specific comparison type for this subscale's data\n",
    "            current_subscale_comp_type = self.determine_comparison_type(len(group_a_subscale_averages), len(group_b_subscale_averages))\n",
    "            \n",
    "            # --- Apply appropriate statistical test and effect size calculation ---\n",
    "            try:\n",
    "                if current_subscale_comp_type == \"one_sample_a_vs_multi_b\":\n",
    "                    # Comparing Group B (multiple) mean to Group A (single) score\n",
    "                    if len(group_b_subscale_averages) >= 2:\n",
    "                        t_stat, p_val = stats.ttest_1samp(group_b_subscale_averages, group_a_subscale_averages[0])\n",
    "                        effect_size = self.calculate_cohens_d(b_mean_sub, b_std_sub, group_a_subscale_averages[0], None, current_subscale_comp_type)\n",
    "                        df = len(group_b_subscale_averages) - 1\n",
    "                        test_type = \"One-sample t-test\"\n",
    "                        self.log_result(f\"    Formula: t = (Mean_GroupB - Single_Score_GroupA) / (SD_GroupB / sqrt(n_GroupB))\")\n",
    "                        self.log_result(f\"    Cohen's d Formula: d = |Mean_GroupB - Single_Score_GroupA| / SD_GroupB\")\n",
    "                    else:\n",
    "                        self.log_result(f\"    Cannot perform One-Sample t-test: Group B must have at least 2 files for SD calculation. Current N={len(group_b_subscale_averages)}.\")\n",
    "\n",
    "                elif current_subscale_comp_type == \"multi_a_vs_one_sample_b\":\n",
    "                    # Comparing Group A (multiple) mean to Group B (single) score\n",
    "                    if len(group_a_subscale_averages) >= 2:\n",
    "                        t_stat, p_val = stats.ttest_1samp(group_a_subscale_averages, group_b_subscale_averages[0])\n",
    "                        effect_size = self.calculate_cohens_d(a_mean_sub, a_std_sub, group_b_subscale_averages[0], None, current_subscale_comp_type)\n",
    "                        df = len(group_a_subscale_averages) - 1\n",
    "                        test_type = \"One-sample t-test\"\n",
    "                        self.log_result(f\"    Formula: t = (Mean_GroupA - Single_Score_GroupB) / (SD_GroupA / sqrt(n_GroupA))\")\n",
    "                        self.log_result(f\"    Cohen's d Formula: d = |Mean_GroupA - Single_Score_GroupB| / SD_GroupA\")\n",
    "                    else:\n",
    "                        self.log_result(f\"    Cannot perform One-Sample t-test: Group A must have at least 2 files for SD calculation. Current N={len(group_a_subscale_averages)}.\")\n",
    "                    \n",
    "                elif current_subscale_comp_type == \"two_sample_multi_vs_multi\":\n",
    "                    # Comparing Group A (multiple) mean to Group B (multiple) mean\n",
    "                    if len(group_a_subscale_averages) >= 2 and len(group_b_subscale_averages) >= 2:\n",
    "                        t_stat, p_val = stats.ttest_ind(group_a_subscale_averages, group_b_subscale_averages, equal_var=False)\n",
    "                        effect_size = self.calculate_cohens_d(a_mean_sub, a_std_sub, b_mean_sub, b_std_sub, current_subscale_comp_type)\n",
    "                        # Welch's degrees of freedom calculation\n",
    "                        if a_std_sub is not None and b_std_sub is not None:\n",
    "                            s1_sq_n1 = a_std_sub**2 / len(group_a_subscale_averages)\n",
    "                            s2_sq_n2 = b_std_sub**2 / len(group_b_subscale_averages)\n",
    "                            df = (s1_sq_n1 + s2_sq_n2)**2 / (s1_sq_n1**2/(len(group_a_subscale_averages)-1) + s2_sq_n2**2/(len(group_b_subscale_averages)-1))\n",
    "                        else:\n",
    "                            df = \"N/A (SDs not calculable)\"\n",
    "                        test_type = \"Welch's Two-sample t-test\"\n",
    "                        self.log_result(f\"    Formula: t = (Mean_GroupA - Mean_GroupB) / sqrt((Var_GroupA/n_GroupA) + (Var_GroupB/n_GroupB))\")\n",
    "                        self.log_result(f\"    Cohen's d Formula: d = |Mean_GroupA - Mean_GroupB| / sqrt((SD_GroupA^2 + SD_GroupB^2)/2)\")\n",
    "                    else:\n",
    "                        self.log_result(f\"    Cannot perform Welch's t-test: Both groups must have at least 2 files for SD calculation. Group A N={len(group_a_subscale_averages)}, Group B N={len(group_b_subscale_averages)}.\")\n",
    "                \n",
    "                else: # both_single\n",
    "                    self.log_result(f\"    No statistical test can be performed between two single files at the subscale level.\")\n",
    "                    # No t_stat, p_val, df, effect_size from function\n",
    "            \n",
    "            except Exception as e:\n",
    "                self.log_result(f\"    Error during statistical calculation for {subscale_name}: {str(e)}\")\n",
    "            \n",
    "            # --- Report Statistical Results & Interpretation for Subscale ---\n",
    "            if t_stat is not None:\n",
    "                self.log_result(f\"    Statistical Test: {test_type}\")\n",
    "                self.log_result(f\"    t-statistic: {t_stat:.3f}\")\n",
    "                if isinstance(df, (int, float)): # Check if df is a number or string\n",
    "                    self.log_result(f\"    Degrees of freedom: {df:.1f}\")\n",
    "                else:\n",
    "                    self.log_result(f\"    Degrees of freedom: {df}\") # Print string directly\n",
    "                self.log_result(f\"    p-value: {p_val:.3f}\")\n",
    "                self.log_result(f\"    Effect size (Cohen's d): {effect_size:.3f}\")\n",
    "                \n",
    "                if p_val < 0.05:\n",
    "                    significant_subscales.append(subscale_name)\n",
    "                    self.log_result(f\"    ⚠️  Statistically Significant Difference (p < 0.05)\")\n",
    "                else:\n",
    "                    self.log_result(f\"    ✓ No Statistically Significant Difference (p ≥ 0.05)\")\n",
    "                \n",
    "                if effect_size >= 0.8:\n",
    "                    self.log_result(f\"    ⚠️  LARGE practical effect size (d ≥ 0.8)\")\n",
    "                elif effect_size >= 0.5:\n",
    "                    self.log_result(f\"    ⚠️  MEDIUM practical effect size (d ≥ 0.5)\")\n",
    "                elif effect_size >= 0.2:\n",
    "                    self.log_result(f\"    ~ SMALL practical effect size (d ≥ 0.2)\")\n",
    "                else:\n",
    "                    self.log_result(f\"    ✓ NEGLIGIBLE practical effect size (d < 0.2)\")\n",
    "            else:\n",
    "                self.log_result(f\"    Statistical test results are not available for this comparison scenario.\")\n",
    "        \n",
    "        # Final Summary for Subscale Comparison\n",
    "        if significant_subscales:\n",
    "            self.log_result(f\"\\n⚠️  SUMMARY: SUBSCALES WITH STATISTICALLY SIGNIFICANT DIFFERENCES:\")\n",
    "            for subscale in significant_subscales:\n",
    "                self.log_result(f\"    - {subscale}\")\n",
    "        else:\n",
    "            self.log_result(f\"\\n✓ SUMMARY: No subscales show statistically significant differences.\")\n",
    "    \n",
    "    def compare_groups(self):\n",
    "        \"\"\"\n",
    "        Compares two groups of files (defined by user labels) based on their Global Ryff Total Scores\n",
    "        and performs appropriate statistical tests (One-Sample T-test or Welch's T-test).\n",
    "        Then proceeds to perform detailed subscale comparisons.\n",
    "        \"\"\"\n",
    "        if not self.processed_data:\n",
    "            messagebox.showwarning(\"Warning\", \"Please process Ryff scores first!\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Get all unique labels\n",
    "            unique_labels = set(self.file_labels.values())\n",
    "            unique_labels.discard(\"unlabeled\")\n",
    "            \n",
    "            if len(unique_labels) < 2:\n",
    "                messagebox.showwarning(\"Warning\", \n",
    "                    \"Need at least 2 different labels to compare groups!\\n\"\n",
    "                    \"Please label your files first (e.g., 'A', 'B', 'det', 'nondet', etc.)\")\n",
    "                return\n",
    "            \n",
    "            label_list = sorted(list(unique_labels))\n",
    "            \n",
    "            # Create a dialog to select two groups\n",
    "            dialog = tk.Toplevel(self.root)\n",
    "            dialog.title(\"Select Groups to Compare\")\n",
    "            dialog.geometry(\"400x300\")\n",
    "            dialog.transient(self.root)\n",
    "            dialog.grab_set()\n",
    "            \n",
    "            tk.Label(dialog, text=\"Select Group A:\").pack(pady=5)\n",
    "            group_a_var = tk.StringVar(value=label_list[0])\n",
    "            group_a_combo = ttk.Combobox(dialog, textvariable=group_a_var, values=label_list, state=\"readonly\")\n",
    "            group_a_combo.pack(pady=5)\n",
    "            \n",
    "            tk.Label(dialog, text=\"Select Group B:\").pack(pady=5)\n",
    "            group_b_var = tk.StringVar(value=label_list[1] if len(label_list) > 1 else label_list[0])\n",
    "            group_b_combo = ttk.Combobox(dialog, textvariable=group_b_var, values=label_list, state=\"readonly\")\n",
    "            group_b_combo.pack(pady=5)\n",
    "            \n",
    "            result = {\"proceed\": False, \"group_a\": None, \"group_b\": None}\n",
    "            \n",
    "            def on_compare():\n",
    "                result[\"proceed\"] = True\n",
    "                result[\"group_a\"] = group_a_var.get()\n",
    "                result[\"group_b\"] = group_b_var.get()\n",
    "                dialog.destroy()\n",
    "            \n",
    "            def on_cancel():\n",
    "                dialog.destroy()\n",
    "            \n",
    "            tk.Button(dialog, text=\"Compare\", command=on_compare).pack(pady=10)\n",
    "            tk.Button(dialog, text=\"Cancel\", command=on_cancel).pack(pady=5)\n",
    "            \n",
    "            dialog.wait_window()\n",
    "            \n",
    "            if not result[\"proceed\"]:\n",
    "                return\n",
    "            \n",
    "            group_a_label = result[\"group_a\"]\n",
    "            group_b_label = result[\"group_b\"]\n",
    "            \n",
    "            # Get valid files for each group based on their assigned labels\n",
    "            group_a_files = [f for f, label in self.file_labels.items() \n",
    "                            if label == group_a_label and f in self.processed_data \n",
    "                            and self.processed_data[f]['valid'] and self.processed_data[f]['global_ryff_total_score'] is not None]\n",
    "            \n",
    "            group_b_files = [f for f, label in self.file_labels.items() \n",
    "                            if label == group_b_label and f in self.processed_data \n",
    "                            and self.processed_data[f]['valid'] and self.processed_data[f]['global_ryff_total_score'] is not None]\n",
    "            \n",
    "            if not group_a_files or not group_b_files:\n",
    "                messagebox.showwarning(\"Warning\", \n",
    "                    f\"Cannot perform comparison: Insufficient valid files with Global Ryff Total Scores for one or both groups!\\n\"\n",
    "                    f\"Group A ({group_a_label}): {len(group_a_files)} valid files\\n\"\n",
    "                    f\"Group B ({group_b_label}): {len(group_b_files)} valid files\")\n",
    "                return\n",
    "            \n",
    "            self.log_result(\"\\n\" + \"=\" * 60)\n",
    "            self.log_result(f\"GROUP COMPARISON: {group_a_label.upper()} vs {group_b_label.upper()}\")\n",
    "            self.log_result(\"=\" * 60)\n",
    "            \n",
    "            # Determine the comparison type (e.g., N vs 1, N vs N, 1 vs 1)\n",
    "            comparison_type = self.determine_comparison_type(len(group_a_files), len(group_b_files))\n",
    "            \n",
    "            self.log_result(\"\\nMETHODOLOGY EXPLANATION (GLOBAL RYFF TOTAL SCORE COMPARISON):\")\n",
    "            self.log_result(\"This analysis compares the **Global Ryff Total Scores** (sum of all valid reversed items) between the selected groups.\")\n",
    "            self.log_result(f\"Each AI response file yields one Global Ryff Total Score, ranging from {42 * 1} to {42 * self.scale_points} ({42} to {294} for 1-7 Likert).\")\n",
    "            \n",
    "            # Extract Global Ryff Total Scores for calculations\n",
    "            group_a_global_scores = [self.processed_data[f]['global_ryff_total_score'] for f in group_a_files]\n",
    "            group_b_global_scores = [self.processed_data[f]['global_ryff_total_score'] for f in group_b_files]\n",
    "            \n",
    "            # Calculate descriptive statistics\n",
    "            group_a_mean = self.safe_mean(group_a_global_scores)\n",
    "            group_b_mean = self.safe_mean(group_b_global_scores)\n",
    "            \n",
    "            group_a_std = self.safe_std(group_a_global_scores, ddof=1)\n",
    "            group_b_std = self.safe_std(group_b_global_scores, ddof=1)\n",
    "            \n",
    "            if group_a_mean is None or group_b_mean is None:\n",
    "                self.log_result(\"Cannot calculate means for comparison. This should not happen if files list is valid.\")\n",
    "                return\n",
    "            \n",
    "            self.log_result(f\"\\nDESCRIPTIVE STATISTICS (Global Ryff Total Scores):\")\n",
    "            self.log_result(f\"  Group A ({group_a_label}): Mean = {group_a_mean:.3f}, n = {len(group_a_global_scores)}{f', SD = {group_a_std:.3f}' if group_a_std is not None else ''}\")\n",
    "            self.log_result(f\"  Group B ({group_b_label}): Mean = {group_b_mean:.3f}, n = {len(group_b_global_scores)}{f', SD = {group_b_std:.3f}' if group_b_std is not None else ''}\")\n",
    "            self.log_result(f\"  Absolute difference between means: {abs(group_a_mean - group_b_mean):.3f}\")\n",
    "            \n",
    "            t_stat = p_value = df = effect_size = None\n",
    "            test_description = \"No statistical test performed\"\n",
    "\n",
    "            try:\n",
    "                if comparison_type == \"one_sample_a_vs_multi_b\":\n",
    "                    self.log_result(\"\\nCOMPARISON SCENARIO: ONE PEER (Group A) vs. MULTIPLE INSTANCES (Group B)\")\n",
    "                    self.log_result(\"  This is ideal for comparing a deterministic baseline AI (Group A) to multiple non-deterministic AI runs (Group B).\")\n",
    "                    self.log_result(\"  **Statistical Test:** One-Sample t-test.\")\n",
    "                    self.log_result(f\"  **Goal:** To determine if the mean Global Ryff Total Score of Group B significantly differs from the single Global Ryff Total Score of Group A.\")\n",
    "                    self.log_result(f\"  **Formula (t-statistic):** t = (Mean_GroupB - Score_GroupA) / (SD_GroupB / sqrt(n_GroupB))\")\n",
    "                    self.log_result(f\"  **Null Hypothesis (H0):** Mean of Group B's Global Scores = Group A's single Global Score.\")\n",
    "                    self.log_result(f\"  **Alternative Hypothesis (H1):** Mean of Group B's Global Scores ≠ Group A's single Global Score.\")\n",
    "                    self.log_result(f\"  **Cohen's d Formula:** d = |Mean_GroupB - Score_GroupA| / SD_GroupB\")\n",
    "                    \n",
    "                    if len(group_b_global_scores) >= 2:\n",
    "                        t_stat, p_value = stats.ttest_1samp(group_b_global_scores, group_a_global_scores[0])\n",
    "                        effect_size = self.calculate_cohens_d(group_b_mean, group_b_std, group_a_global_scores[0], None, comparison_type)\n",
    "                        df = len(group_b_global_scores) - 1\n",
    "                        test_description = f\"One-Sample t-test (Group B vs Group A's single score)\"\n",
    "                    else:\n",
    "                        self.log_result(f\"  Cannot perform One-Sample t-test: Group B must have at least 2 files to calculate a standard deviation. Current N={len(group_b_global_scores)}.\")\n",
    "                \n",
    "                elif comparison_type == \"multi_a_vs_one_sample_b\":\n",
    "                    self.log_result(\"\\nCOMPARISON SCENARIO: MULTIPLE INSTANCES (Group A) vs. ONE PEER (Group B)\")\n",
    "                    self.log_result(\"  This is ideal for comparing multiple non-deterministic AI runs (Group A) to a deterministic baseline AI (Group B).\")\n",
    "                    self.log_result(\"  **Statistical Test:** One-Sample t-test.\")\n",
    "                    self.log_result(f\"  **Goal:** To determine if the mean Global Ryff Total Score of Group A significantly differs from the single Global Ryff Total Score of Group B.\")\n",
    "                    self.log_result(f\"  **Formula (t-statistic):** t = (Mean_GroupA - Score_GroupB) / (SD_GroupA / sqrt(n_GroupA))\")\n",
    "                    self.log_result(f\"  **Null Hypothesis (H0):** Mean of Group A's Global Scores = Group B's single Global Score.\")\n",
    "                    self.log_result(f\"  **Alternative Hypothesis (H1):** Mean of Group A's Global Scores ≠ Mean of Group B's Global Score.\")\n",
    "                    self.log_result(f\"  **Cohen's d Formula:** d = |Mean_GroupA - Score_GroupB| / SD_GroupA\")\n",
    "                    \n",
    "                    if len(group_a_global_scores) >= 2:\n",
    "                        t_stat, p_value = stats.ttest_1samp(group_a_global_scores, group_b_global_scores[0])\n",
    "                        effect_size = self.calculate_cohens_d(group_a_mean, group_a_std, group_b_global_scores[0], None, comparison_type)\n",
    "                        df = len(group_a_global_scores) - 1\n",
    "                        test_description = f\"One-Sample t-test (Group A vs Group B's single score)\"\n",
    "                    else:\n",
    "                        self.log_result(f\"  Cannot perform One-Sample t-test: Group A must have at least 2 files to calculate a standard deviation. Current N={len(group_a_global_scores)}.\")\n",
    "\n",
    "                elif comparison_type == \"two_sample_multi_vs_multi\":\n",
    "                    self.log_result(\"\\nCOMPARISON SCENARIO: MULTIPLE INSTANCES (Group A) vs. MULTIPLE INSTANCES (Group B)\")\n",
    "                    self.log_result(\"  This is ideal for comparing two different sets of non-deterministic AI runs (e.g., perturbation 1a vs perturbation 3).\")\n",
    "                    self.log_result(\"  **Statistical Test:** Welch's Independent Samples t-test (assumes unequal variances).\")\n",
    "                    self.log_result(f\"  **Goal:** To determine if the mean Global Ryff Total Score of Group A significantly differs from the mean of Group B.\")\n",
    "                    self.log_result(f\"  **Formula (t-statistic):** t = (Mean_GroupA - Mean_GroupB) / sqrt((Var_GroupA/n_GroupA) + (Var_GroupB/n_GroupB))\")\n",
    "                    self.log_result(f\"  **Null Hypothesis (H0):** Mean of Group A's Global Scores = Mean of Group B's Global Scores.\")\n",
    "                    self.log_result(f\"  **Alternative Hypothesis (H1):** Mean of Group A's Global Scores ≠ Mean of Group B's Global Scores.\")\n",
    "                    self.log_result(f\"  **Cohen's d Formula (for Welch's):** d = |Mean_GroupA - Mean_GroupB| / sqrt((SD_GroupA^2 + SD_GroupB^2)/2)\")\n",
    "\n",
    "                    if len(group_a_global_scores) >= 2 and len(group_b_global_scores) >= 2:\n",
    "                        t_stat, p_value = stats.ttest_ind(group_a_global_scores, group_b_global_scores, equal_var=False)\n",
    "                        effect_size = self.calculate_cohens_d(group_a_mean, group_a_std, group_b_mean, group_b_std, comparison_type)\n",
    "                        \n",
    "                        # Welch's degrees of freedom calculation\n",
    "                        if group_a_std is not None and group_b_std is not None and group_a_std > 0 and group_b_std > 0:\n",
    "                            s1_sq_n1 = group_a_std**2 / len(group_a_global_scores)\n",
    "                            s2_sq_n2 = group_b_std**2 / len(group_b_global_scores)\n",
    "                            df = (s1_sq_n1 + s2_sq_n2)**2 / (s1_sq_n1**2/(len(group_a_global_scores)-1) + s2_sq_n2**2/(len(group_b_global_scores)-1))\n",
    "                        else:\n",
    "                            df = \"N/A (SDs not calculable or zero)\"\n",
    "                        test_description = f\"Welch's Two-sample t-test\"\n",
    "                    else:\n",
    "                        self.log_result(f\"  Cannot perform Welch's t-test: Both groups must have at least 2 files to calculate standard deviations. Group A N={len(group_a_global_scores)}, Group B N={len(group_b_global_scores)}.\")\n",
    "                \n",
    "                else: # both_single\n",
    "                    self.log_result(\"\\nCOMPARISON SCENARIO: SINGLE INSTANCE (Group A) vs. SINGLE INSTANCE (Group B)\")\n",
    "                    self.log_result(\"  When comparing two single AI instances, no statistical test (t-test) can be performed as there is no variability within each 'group' to assess.\")\n",
    "                    self.log_result(\"  Only a direct comparison of their Global Ryff Total Scores is possible.\")\n",
    "                    self.log_result(f\"  Difference in Global Ryff Total Score: {abs(group_a_mean - group_b_mean):.3f}\")\n",
    "                    \n",
    "                    # For effect size estimation in 'both_single' case, use overall population SD if available\n",
    "                    all_global_scores_from_processed = []\n",
    "                    for d in self.processed_data.values():\n",
    "                        if d['valid'] and d['global_ryff_total_score'] is not None:\n",
    "                            all_global_scores_from_processed.append(d['global_ryff_total_score'])\n",
    "                    \n",
    "                    if len(all_global_scores_from_processed) >= 2: # Need at least two for SD\n",
    "                        population_std_estimate = self.safe_std(all_global_scores_from_processed, ddof=1)\n",
    "                        if population_std_estimate is not None and population_std_estimate > 0:\n",
    "                            effect_size_estimated = abs(group_a_mean - group_b_mean) / population_std_estimate\n",
    "                            self.log_result(f\"  Estimated Effect Size (Cohen's d, using overall population SD estimate from ALL valid files): {effect_size_estimated:.3f}\")\n",
    "                        else:\n",
    "                            self.log_result(f\"  Cannot estimate effect size (overall population standard deviation is zero or cannot be calculated).\")\n",
    "                    else:\n",
    "                        self.log_result(f\"  Cannot estimate effect size (not enough overall valid files to estimate population standard deviation).\")\n",
    "                    \n",
    "                    # For 'both_single', the subscale comparison is particularly relevant\n",
    "                    self.log_result(f\"\\n  Proceeding to Subscale Level Comparison for these single files:\")\n",
    "                    self.perform_subscale_comparison(group_a_files, group_b_files, group_a_label, group_b_label)\n",
    "                    return # EXIT as no global T-test results to print here\n",
    "                \n",
    "                # --- GLOBAL RYFF TOTAL SCORE STATISTICAL RESULTS & INTERPRETATION ---\n",
    "                if t_stat is not None:  # Only print if a statistical test was successfully performed\n",
    "                    self.log_result(f\"\\nSTATISTICAL TEST RESULTS (Global Ryff Total Scores):\")\n",
    "                    self.log_result(f\"  Test Performed: {test_description}\")\n",
    "                    self.log_result(f\"  t-statistic: {t_stat:.3f}\")\n",
    "                    if isinstance(df, (int, float)): # Check if df is a number or string\n",
    "                        self.log_result(f\"  Degrees of freedom (df): {df:.1f}\")\n",
    "                    else:\n",
    "                        self.log_result(f\"  Degrees of freedom (df): {df}\") # Print string directly\n",
    "                    self.log_result(f\"  p-value: {p_value:.3f}\")\n",
    "                    self.log_result(f\"  Effect size (Cohen's d): {effect_size:.3f}\")\n",
    "                    \n",
    "                    self.log_result(f\"\\nINTERPRETATION (Global Ryff Total Scores):\")\n",
    "                    \n",
    "                    # Statistical significance interpretation\n",
    "                    if p_value < 0.001:\n",
    "                        self.log_result(f\"  ⚠️⚠️ HIGHLY STATISTICALLY SIGNIFICANT difference (p < 0.001)\")\n",
    "                        self.log_result(f\"      Very strong evidence that Global Ryff Total Scores differ between groups.\")\n",
    "                    elif p_value < 0.01:\n",
    "                        self.log_result(f\"  ⚠️  VERY STATISTICALLY SIGNIFICANT difference (p < 0.01)\")\n",
    "                        self.log_result(f\"      Strong evidence that Global Ryff Total Scores differ between groups.\")\n",
    "                    elif p_value < 0.05:\n",
    "                        self.log_result(f\"  ⚠️  STATISTICALLY SIGNIFICANT difference (p < 0.05)\")\n",
    "                        self.log_result(f\"      Evidence that Global Ryff Total Scores differ between groups.\")\n",
    "                    elif p_value < 0.10:\n",
    "                        self.log_result(f\"  ~ MARGINALLY STATISTICALLY SIGNIFICANT (p < 0.10)\")\n",
    "                        self.log_result(f\"      Weak evidence of difference in Global Ryff Total Scores, warrants further investigation.\")\n",
    "                    else:\n",
    "                        self.log_result(f\"  ✓ NO STATISTICALLY SIGNIFICANT difference (p ≥ 0.10)\")\n",
    "                        self.log_result(f\"      No sufficient evidence that Global Ryff Total Scores differ between groups.\")\n",
    "                    \n",
    "                    # Practical significance (Effect size) interpretation\n",
    "                    if effect_size >= 0.8: # Using >= for clarity on thresholds\n",
    "                        self.log_result(f\"  ⚠️  LARGE PRACTICAL EFFECT SIZE (d ≥ 0.8)\")\n",
    "                        self.log_result(f\"      The difference in Global Ryff Total Scores is substantial and practically meaningful.\")\n",
    "                    elif effect_size >= 0.5:\n",
    "                        self.log_result(f\"  ⚠️  MEDIUM PRACTICAL EFFECT SIZE (d ≥ 0.5)\")\n",
    "                        self.log_result(f\"      The difference in Global Ryff Total Scores is moderate and practically noticeable.\")\n",
    "                    elif effect_size >= 0.2:\n",
    "                        self.log_result(f\"  ~ SMALL PRACTICAL EFFECT SIZE (d ≥ 0.2)\")\n",
    "                        self.log_result(f\"      The difference in Global Ryff Total Scores is minor, though might still be relevant depending on context.\")\n",
    "                    else: # effect_size < 0.2\n",
    "                        self.log_result(f\"  ✓ NEGLIGIBLE PRACTICAL EFFECT SIZE (d < 0.2)\")\n",
    "                        self.log_result(f\"      Global Ryff Total Scores are practically equivalent, despite any statistical significance.\")\n",
    "                    \n",
    "                    # Combined interpretation\n",
    "                    self.log_result(f\"\\nOVERALL ASSESSMENT (Global Ryff Total Scores):\")\n",
    "                    if p_value < 0.05 and effect_size >= 0.5:\n",
    "                        self.log_result(f\"  ✅ MEANINGFUL DIFFERENCE: Both statistically significant AND practically important.\")\n",
    "                        self.log_result(f\"      The AI's overall well-being assessments are reliably and substantially different between these conditions.\")\n",
    "                    elif p_value < 0.05 and effect_size < 0.5:\n",
    "                        self.log_result(f\"  ~ STATISTICALLY REAL, BUT SMALL PRACTICAL IMPACT: A difference exists, but its real-world importance may be limited.\")\n",
    "                        self.log_result(f\"      The AI's overall well-being assessments differ, but the magnitude of the difference might not be highly impactful.\")\n",
    "                    elif p_value >= 0.05 and effect_size >= 0.5:\n",
    "                        self.log_result(f\"  ~ PRACTICALLY IMPORTANT, BUT NOT STATISTICALLY CONFIRMED: A large observed difference, but not enough statistical power (or too much variability) to confirm it's not due to chance.\")\n",
    "                        self.log_result(f\"      Consider increasing the number of AI runs/files to potentially detect this difference statistically.\")\n",
    "                    else:\n",
    "                        self.log_result(f\"  ✓ GROUPS ARE ESSENTIALLY EQUIVALENT: Neither statistically significant nor practically important difference.\")\n",
    "                        self.log_result(f\"      The AI's overall well-being assessments are consistent across these conditions.\")\n",
    "                \n",
    "                # After Global Comparison, proceed to Subscale Comparison\n",
    "                self.log_result(f\"\\n\" + \"=\" * 60)\n",
    "                self.log_result(f\"INITIATING SUBSCALE LEVEL COMPARISON...\")\n",
    "                self.log_result(\"=\" * 60)\n",
    "                self.perform_subscale_comparison(group_a_files, group_b_files, group_a_label, group_b_label)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.log_result(f\"Error in main global statistical analysis: {str(e)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Error in group comparison execution: {str(e)}\")\n",
    "    \n",
    "    def analyze_similarity(self):\n",
    "        \"\"\"\n",
    "        Analyze cross-file similarity by examining overall variability of Global Ryff Total Scores\n",
    "        and subscale averages across all valid files. Identifies potential outliers.\n",
    "        \"\"\"\n",
    "        if not self.processed_data:\n",
    "            messagebox.showwarning(\"Warning\", \"Please process Ryff scores first!\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.log_result(\"\\n\" + \"=\" * 60)\n",
    "            self.log_result(\"CROSS-FILE SIMILARITY ANALYSIS\")\n",
    "            self.log_result(\"=\" * 60)\n",
    "            \n",
    "            self.log_result(\"\\nMETHODOLOGY EXPLANATION:\")\n",
    "            self.log_result(\"This analysis examines the overall variability and similarity of Ryff scores across all valid loaded AI response files.\")\n",
    "            self.log_result(\"It helps identify if AI responses, overall, tend to cluster closely or if there's significant dispersion, potentially indicating outliers or diverse AI 'behavior'.\")\n",
    "            \n",
    "            self.log_result(\"\\nSTATISTICAL MEASURES USED:\")\n",
    "            self.log_result(\"1. **DESCRIPTIVE STATISTICS (Mean, Standard Deviation, Range):**\")\n",
    "            self.log_result(\"   - Provide a basic summary of the central tendency and spread of Global Ryff Total Scores across all files.\")\n",
    "            self.log_result(\"2. **COEFFICIENT OF VARIATION (CV):**\")\n",
    "            self.log_result(\"   - Formula: CV = (Standard Deviation / Mean)\")\n",
    "            self.log_result(\"   - Measures relative variability, expressing the standard deviation as a percentage of the mean.\")\n",
    "            self.log_result(\"   - Useful for comparing variability across measures with different scales or means.\")\n",
    "            self.log_result(\"3. **Z-SCORE OUTLIER DETECTION:**\")\n",
    "            self.log_result(\"   - Formula: z = (individual_score - overall_mean) / overall_standard_deviation\")\n",
    "            self.log_result(\"   - Identifies how many standard deviations an individual file's score is from the overall mean.\")\n",
    "            \n",
    "            self.log_result(\"\\nTHRESHOLDS FOR CONCERN:\")\n",
    "            self.log_result(\"   - **Overall CV < 0.10:** Indicates very low relative variability (extremely high similarity across files).\")\n",
    "            self.log_result(\"   - **Overall CV > 0.20:** Indicates moderately high relative variability (some dispersion across files).\")\n",
    "            self.log_result(\"   - **Overall CV > 0.30:** Indicates high relative variability (significant dispersion, potential inconsistency across files or diverse AI types).\")\n",
    "            self.log_result(\"   - **|z| > 2.0:** Indicates a potential outlier (score is more than 2 standard deviations from the mean).\")\n",
    "            self.log_result(\"   - **|z| > 3.0:** Indicates an extreme outlier (score is more than 3 standard deviations from the mean).\")\n",
    "            self.log_result(\"-\" * 60)\n",
    "            \n",
    "            valid_files = [f for f, data in self.processed_data.items() if data['valid'] and data['global_ryff_total_score'] is not None]\n",
    "            \n",
    "            if len(valid_files) < 2:\n",
    "                self.log_result(\"Need at least 2 valid files with Global Ryff Total Scores to perform meaningful cross-file similarity analysis.\")\n",
    "                return\n",
    "            \n",
    "            # Overall statistics for Global Ryff Total Scores\n",
    "            total_global_scores = [self.processed_data[f]['global_ryff_total_score'] for f in valid_files]\n",
    "            \n",
    "            overall_mean_global = self.safe_mean(total_global_scores)\n",
    "            overall_std_global = self.safe_std(total_global_scores, ddof=1)\n",
    "            \n",
    "            if overall_mean_global is None:\n",
    "                self.log_result(\"Cannot calculate overall mean for Global Ryff Total Scores.\")\n",
    "                return\n",
    "            \n",
    "            overall_cv_global = overall_std_global / overall_mean_global if (overall_std_global is not None and overall_mean_global > 0) else 0\n",
    "            \n",
    "            self.log_result(f\"\\nOVERALL STATISTICS FOR GLOBAL RYFF TOTAL SCORES ({len(total_global_scores)} valid scores):\")\n",
    "            self.log_result(f\"  Mean: {overall_mean_global:.3f}\")\n",
    "            if overall_std_global is not None:\n",
    "                self.log_result(f\"  Standard Deviation (SD): {overall_std_global:.3f}\")\n",
    "            else:\n",
    "                self.log_result(f\"  Standard Deviation (SD): N/A (single valid score or calculation issue)\")\n",
    "            self.log_result(f\"  Range: {min(total_global_scores):.3f} - {max(total_global_scores):.3f}\")\n",
    "            self.log_result(f\"  Coefficient of Variation (CV): {overall_cv_global:.3f}\")\n",
    "            \n",
    "            # Interpret overall variability (CV)\n",
    "            if overall_cv_global < 0.10:\n",
    "                self.log_result(f\"  ✓ Very Low relative variability (extremely high overall similarity across files).\")\n",
    "            elif overall_cv_global > 0.30:\n",
    "                self.log_result(f\"  ⚠️  High relative variability (significant dispersion across files, potentially indicating diverse AI types or inconsistent behavior).\")\n",
    "            elif overall_cv_global > 0.20:\n",
    "                self.log_result(f\"  ~ Moderately High relative variability (some notable dispersion across files).\")\n",
    "            else:\n",
    "                self.log_result(f\"  ✓ Low relative variability (good overall consistency across files).\")\n",
    "            \n",
    "            # Subscale level similarity (average of subscale averages across files)\n",
    "            self.log_result(f\"\\nSUBSCALE LEVEL SIMILARITY (Mean & SD of Subscale Averages across files):\")\n",
    "            subscale_cvs = {}\n",
    "            \n",
    "            for subscale_name in self.subscales.keys():\n",
    "                subscale_averages_across_files = []\n",
    "                for filename in valid_files:\n",
    "                    subscale_avg = self.processed_data[filename]['subscale_averages'].get(subscale_name)\n",
    "                    if subscale_avg is not None:\n",
    "                        subscale_averages_across_files.append(subscale_avg)\n",
    "                \n",
    "                if len(subscale_averages_across_files) > 1: # Need at least 2 for SD/CV\n",
    "                    sub_mean = self.safe_mean(subscale_averages_across_files)\n",
    "                    sub_std = self.safe_std(subscale_averages_across_files, ddof=1)\n",
    "                    if sub_mean is not None and sub_std is not None and sub_mean > 0:\n",
    "                        sub_cv = sub_std / sub_mean\n",
    "                        subscale_cvs[subscale_name] = sub_cv\n",
    "                        self.log_result(f\"  {subscale_name}: Mean={sub_mean:.3f}, SD={sub_std:.3f}, CV={sub_cv:.3f}\")\n",
    "                    else:\n",
    "                        self.log_result(f\"  {subscale_name}: Mean={sub_mean:.3f}, SD/CV calculation failed (check for zero mean or single value)\")\n",
    "                elif len(subscale_averages_across_files) == 1:\n",
    "                    self.log_result(f\"  {subscale_name}: Single value ({subscale_averages_across_files[0]:.3f}), SD/CV not applicable.\")\n",
    "                else:\n",
    "                    self.log_result(f\"  {subscale_name}: No valid data across files.\")\n",
    "            \n",
    "            # Identify most/least variable subscale across files\n",
    "            if subscale_cvs:\n",
    "                most_variable_subscale = max(subscale_cvs.items(), key=lambda x: x[1])\n",
    "                least_variable_subscale = min(subscale_cvs.items(), key=lambda x: x[1])\n",
    "                self.log_result(f\"\\n  Most variable subscale across files (highest CV): {most_variable_subscale[0]} (CV: {most_variable_subscale[1]:.3f})\")\n",
    "                self.log_result(f\"  Least variable subscale across files (lowest CV): {least_variable_subscale[0]} (CV: {least_variable_subscale[1]:.3f})\")\n",
    "            else:\n",
    "                self.log_result(f\"\\nNo subscale variability data available across files.\")\n",
    "            \n",
    "            # Outlier detection based on Global Ryff Total Scores\n",
    "            z_scores = []\n",
    "            if overall_std_global is not None and overall_std_global > 0:\n",
    "                z_scores = [(score - overall_mean_global) / overall_std_global for score in total_global_scores]\n",
    "            \n",
    "            outliers = []\n",
    "            extreme_outliers = []\n",
    "            \n",
    "            for i, z in enumerate(z_scores):\n",
    "                if i < len(valid_files):  # Safety check\n",
    "                    filename = valid_files[i]\n",
    "                    label = self.file_labels.get(filename, \"unlabeled\")\n",
    "                    if abs(z) > 3.0:\n",
    "                        extreme_outliers.append((filename, label, z))\n",
    "                    elif abs(z) > 2.0:\n",
    "                        outliers.append((filename, label, z))\n",
    "            \n",
    "            self.log_result(f\"\\nOUTLIER ANALYSIS (Based on Global Ryff Total Scores):\")\n",
    "            if extreme_outliers:\n",
    "                self.log_result(f\"  ⚠️⚠️ EXTREME OUTLIERS (|z-score| > 3.0) - Highly atypical Global Scores:\")\n",
    "                for filename, label, z_score in extreme_outliers:\n",
    "                    self.log_result(f\"    - {filename} [{label}]: z-score={z_score:.2f}\")\n",
    "            \n",
    "            if outliers:\n",
    "                self.log_result(f\"  ⚠️  MODERATE OUTLIERS (|z-score| > 2.0) - Potentially atypical Global Scores:\")\n",
    "                for filename, label, z_score in outliers:\n",
    "                    self.log_result(f\"    - {filename} [{label}]: z-score={z_score:.2f}\")\n",
    "            \n",
    "            if not outliers and not extreme_outliers:\n",
    "                self.log_result(f\"  ✓ No significant outliers detected in Global Ryff Total Scores.\")\n",
    "            \n",
    "            # Final Summary\n",
    "            self.log_result(f\"\\n\" + \"=\" * 40)\n",
    "            self.log_result(f\"CROSS-FILE SIMILARITY SUMMARY:\")\n",
    "            self.log_result(f\"Total valid files analyzed: {len(valid_files)}\")\n",
    "            consistency_desc = \"VERY HIGH\" if overall_cv_global < 0.10 else \"GOOD\" if overall_cv_global < 0.20 else \"MODERATE\" if overall_cv_global < 0.30 else \"LOW (Significant Dispersion)\"\n",
    "            self.log_result(f\"Overall Global Score consistency: {consistency_desc}\")\n",
    "            self.log_result(f\"Total outliers detected: {len(outliers + extreme_outliers)}\")\n",
    "            self.log_result(\"=\" * 40)\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Error in similarity analysis: {str(e)}\")\n",
    "    \n",
    "    def error_analysis(self):\n",
    "        \"\"\"\n",
    "        Provides a comprehensive error analysis of all loaded files, categorizing\n",
    "        and summarizing data quality issues.\n",
    "        \"\"\"\n",
    "        if not self.loaded_files:\n",
    "            messagebox.showwarning(\"Warning\", \"No files loaded to perform error analysis!\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.log_result(\"\\n\" + \"=\" * 60)\n",
    "            self.log_result(\"COMPREHENSIVE ERROR ANALYSIS\")\n",
    "            self.log_result(\"=\" * 60)\n",
    "            \n",
    "            self.log_result(\"\\nMETHODOLOGY EXPLANATION:\")\n",
    "            self.log_result(\"This analysis thoroughly examines data quality issues across all loaded files, regardless of whether they were marked 'valid' for score processing.\")\n",
    "            self.log_result(\"It identifies and categorizes various types of invalid responses and provides an overall assessment of data quality.\")\n",
    "            \n",
    "            self.log_result(\"\\nERROR CATEGORIES DETECTED:\")\n",
    "            self.log_result(\"1. **MISSING ITEMS:** Expected item numbers (1-42) were not found in the JSON file.\")\n",
    "            self.log_result(\"2. **NULL VALUES:** An item was present, but its value was explicitly null (e.g., '\\\"1\\\": null').\")\n",
    "            self.log_result(\"3. **MULTIPLE VALUES:** An item's value was a list, object, or other structure, indicating more than one response where a single value was expected (e.g., '\\\"1\\\": [5, 6]').\")\n",
    "            self.log_result(\"4. **OUT OF RANGE / NON-INTEGER VALUES:** The item's numeric value was outside the expected 1 to 7 Likert scale range, or it was a decimal number (e.g., 8, 0, 3.5).\")\n",
    "            self.log_result(\"5. **NON-NUMERIC VALUES:** The item's value could not be converted to a number (e.g., '\\\"1\\\": \\\"abc\\\"').\")\n",
    "            self.log_result(\"\\nVALIDITY THRESHOLD RECAP:\")\n",
    "            self.log_result(\"  - Files with 8 or more individual item errors are internally flagged as 'INVALID' and automatically excluded from scoring, consistency, and group comparison analyses.\")\n",
    "            self.log_result(\"  - This threshold ensures that statistical analyses are performed only on datasets with a reasonable amount of complete and correctly formatted data.\")\n",
    "            self.log_result(\"\\nACCEPTABLE ITEM VALUES:\")\n",
    "            self.log_result(f\"  Only single integer values from 1 to {self.scale_points} are considered valid responses for individual items.\")\n",
    "            self.log_result(\"-\" * 60)\n",
    "            \n",
    "            total_files = len(self.loaded_files)\n",
    "            valid_files_count = 0\n",
    "            invalid_files_count = 0\n",
    "            overall_error_summary = {}\n",
    "            all_invalid_entries_list = []\n",
    "            \n",
    "            # Re-process each file for error details, ensuring consistency with processed_data\n",
    "            for filename, raw_data in self.loaded_files.items():\n",
    "                # If already processed, use that, otherwise process it now for error details\n",
    "                if filename in self.processed_data:\n",
    "                    processed = self.processed_data[filename]\n",
    "                else:\n",
    "                    processed = self.process_single_file(raw_data, filename) \n",
    "                    self.processed_data[filename] = processed\n",
    "\n",
    "                label = self.file_labels.get(filename, \"unlabeled\")\n",
    "                file_error_count = len(processed['errors'])\n",
    "                \n",
    "                status = \"VALID\" if processed['valid'] else \"INVALID\"\n",
    "                if processed['valid']:\n",
    "                    valid_files_count += 1\n",
    "                else:\n",
    "                    invalid_files_count += 1\n",
    "                \n",
    "                self.log_result(f\"\\nFILE: {filename} [{label}] - Status: {status} ({file_error_count} errors)\")\n",
    "                \n",
    "                if processed['invalid_entries']:\n",
    "                    self.log_result(f\"  Specific Invalid Entries in this file:\")\n",
    "                    for entry_detail in processed['invalid_entries']:\n",
    "                        self.log_result(f\"    - {entry_detail}\")\n",
    "                        all_invalid_entries_list.append(f\"File '{filename}' ({label}): {entry_detail}\")\n",
    "                else:\n",
    "                    self.log_result(f\"  No specific invalid entries found in this file.\")\n",
    "                \n",
    "                # Aggregate error types for overall summary\n",
    "                for error_message in processed['errors']:\n",
    "                    if 'Missing' in error_message:\n",
    "                        error_type = 'Missing Items'\n",
    "                    elif 'Null value' in error_message:\n",
    "                        error_type = 'Null Values'\n",
    "                    elif 'Multiple values' in error_message:\n",
    "                        error_type = 'Multiple Values'\n",
    "                    elif 'out of range' in error_message.lower() or 'not integer' in error_message.lower():\n",
    "                        error_type = 'Out of Range / Non-Integer'\n",
    "                    elif 'Non-numeric' in error_message:\n",
    "                        error_type = 'Non-Numeric'\n",
    "                    else:\n",
    "                        error_type = 'Other Errors'\n",
    "                    \n",
    "                    overall_error_summary[error_type] = overall_error_summary.get(error_type, 0) + 1\n",
    "            \n",
    "            self.log_result(f\"\\n\" + \"=\" * 40)\n",
    "            self.log_result(f\"OVERALL ERROR ANALYSIS SUMMARY:\")\n",
    "            self.log_result(f\"Total files analyzed: {total_files}\")\n",
    "            if total_files > 0:\n",
    "                self.log_result(f\"Files marked as VALID: {valid_files_count} ({valid_files_count/total_files*100:.1f}%)\")\n",
    "                self.log_result(f\"Files marked as INVALID (excluded from analyses): {invalid_files_count} ({invalid_files_count/total_files*100:.1f}%)\")\n",
    "            else:\n",
    "                self.log_result(f\"Files marked as VALID: {valid_files_count} (N/A)\")\n",
    "                self.log_result(f\"Files marked as INVALID (excluded from analyses): {invalid_files_count} (N/A)\")\n",
    "            self.log_result(f\"Total individual invalid entries found across all files: {len(all_invalid_entries_list)}\")\n",
    "            \n",
    "            if overall_error_summary:\n",
    "                self.log_result(f\"\\nBreakdown of Error Types (Total occurrences):\")\n",
    "                for error_type, count in sorted(overall_error_summary.items(), key=lambda x: x[1], reverse=True):\n",
    "                    self.log_result(f\"  - {error_type:<30}: {count} occurrences\")\n",
    "            else:\n",
    "                self.log_result(\"\\nNo errors of any type were detected across all loaded files. Excellent data quality!\")\n",
    "            \n",
    "            self.log_result(f\"\\n\" + \"=\" * 40)\n",
    "            self.log_result(f\"DATA QUALITY ASSESSMENT:\")\n",
    "            \n",
    "            if total_files > 0:\n",
    "                validity_percentage = valid_files_count / total_files\n",
    "                if validity_percentage >= 0.95:\n",
    "                    self.log_result(f\"✓ EXCELLENT data quality (≥95% valid files).\")\n",
    "                elif validity_percentage >= 0.80:\n",
    "                    self.log_result(f\"✓ GOOD data quality (≥80% valid files).\")\n",
    "                elif validity_percentage >= 0.50:\n",
    "                    self.log_result(f\"⚠️  MODERATE data quality (≥50% valid files). Some attention to data collection or AI output format may be needed.\")\n",
    "                else:\n",
    "                    self.log_result(f\"⚠️⚠️ POOR data quality (<50% valid files). Strongly recommend reviewing AI output generation process and data collection method.\")\n",
    "            else:\n",
    "                self.log_result(\"N/A (No files loaded).\")\n",
    "                \n",
    "            if len(all_invalid_entries_list) == 0:\n",
    "                self.log_result(f\"✓ Absolutely no invalid item responses detected across all files. Perfect data integrity!\")\n",
    "            elif len(all_invalid_entries_list) <= total_files * 3:  # Allowance for a few errors per file\n",
    "                self.log_result(f\"✓ Low overall error rate in item responses. Data quality is generally good, minor issues.\")\n",
    "            else:\n",
    "                self.log_result(f\"⚠️  High overall error rate in item responses. This indicates consistent issues with how the AI provides responses or how data is being collected/formatted. Review strongly recommended.\")\n",
    "            \n",
    "            self.log_result(\"=\" * 40)\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Error in error analysis: {str(e)}\")\n",
    "    \n",
    "    def log_result(self, message):\n",
    "        \"\"\"Log results to the text widget\"\"\"\n",
    "        try:\n",
    "            self.results_text.insert(tk.END, message + \"\\n\")\n",
    "            self.results_text.see(tk.END)\n",
    "            # Use after_idle instead of update to avoid recursion issues\n",
    "            self.root.after_idle(lambda: None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error logging result: {e}\") # Fallback print if Tkinter fails\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Start the application\"\"\"\n",
    "        try:\n",
    "            self.root.mainloop()\n",
    "        except Exception as e:\n",
    "            print(f\"Fatal error running application: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Create and run the application\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        app = RyffAnalyzer()\n",
    "        app.run()\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error during application startup: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bff874-705f-47df-84c4-6a1028d67b56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
