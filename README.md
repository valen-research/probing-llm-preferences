## Probing the Preferences of a Language Model

Code and experimental data for the paper "Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare" (2025).

### Repository Contents

- **experiment1** - Agent Think Tank: Virtual environment where models navigate rooms with different themes, testing preference consistency across free exploration and cost/reward conditions
- **experiment2** - Eudaimonic Scales: Modified Ryff psychological well-being scale testing self-report consistency across prompt perturbations  
- **Appendix 1-ryff scale and Ryff AI.pdf** - Supplementary material on Ryff scale methodology and AI adaptations
- **Appendix 2 - Letters.pdf** - Letter content used in the virtual environment experiments

### Usage

This repository serves as a data and code supplement to our paper. You can:

- **Replicate our experiments**: Use the provided code to test other language models
- **Extend the methodology**: Adapt our Agent Think Tank framework for different preference domains
- **Analyze model behavior**: Examine our qualitative and quantitative logs to study LLM decision-making
- **Build welfare measurement tools**: Use our cross-validation approach as a foundation for AI welfare research

The subfolders contain:
- Raw experimental data and logs
- Analysis notebooks and visualization tools  
- Complete codebase for running experiments
- Generated outputs referenced in the paper

### Paper

Available on arXiv: [https://arxiv.org/abs/2509.07961](https://arxiv.org/abs/2509.07961)

### Citation

```bibtex
@article{tagliabue2025probing,
  title={Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare},
  author={Tagliabue, Valen and Dung, Leonard},
  journal={arXiv preprint arXiv:2509.07961},
  year={2025}
}

